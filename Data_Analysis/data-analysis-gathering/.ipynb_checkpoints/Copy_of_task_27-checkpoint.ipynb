{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cY_pbpMg8T2g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REyPSoAX8YRm"
   },
   "source": [
    "### Problem-1:\n",
    "\n",
    "You are given a SQL file link: https://drive.google.com/file/d/1WFt7B84LTHhMueoKmz8W-PRo7xXqmZf3/view?usp=share_link. Read the data by using the file and store it in a excel file. In this data, there are 3 tables named \"invoices\", \"order_leads\" and \"sales_sql\". So create 3 sheets to your excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cg4lTZ1WENAB",
    "outputId": "c3a95b28-1e14-48b4-cc33-7e8c3d619db2"
   },
   "outputs": [],
   "source": [
    "!pip install mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjgZ3pDV8Xza"
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "supermarket = mysql.connector.connect(host='localhost',user='root',password='',database='supermarket')\n",
    "invoices = pd.read_sql_query('SELECT * FROM invoices',supermarket)\n",
    "orderleads = pd.read_sql_query('SELECT * FROM orderleads',supermarket)\n",
    "salesteam = pd.read_sql_query('SELECT * FROM salesteam',supermarket)\n",
    "\n",
    "with pd.ExcelWriter('supermarket.xlsx') as writer:\n",
    "    invoices.to_excel(writer,sheet_name='invoices')\n",
    "    orderleads.to_excel(writer,sheet_name='orderleads')\n",
    "    salesteam.to_excel(writer,sheet_name='salesteam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixwn5tD08dDV"
   },
   "source": [
    "### Problem-2\n",
    "\n",
    "Go to the site: https://rapidapi.com/wirefreethought/api/geodb-cities. From here, you have to grab the API and have to choose proper routes to get the cities of different countries. After getting the right API, hit that API and create a dataframe of all the cities that you can get by using the API. Then store the dataframe to a SQL. If you need to create an account or have to subscribe, then do that (it has free subscription but has some limitations. Use that free subscription and modify your accordingly to get all the data).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# url = 'https://wft-geo-db.p.rapidapi.com/v1/geo/countries'\n",
    "# headers = {\n",
    "#     'x-rapidapi-host': 'wft-geo-db.p.rapidapi.com',\n",
    "#     'x-rapidapi-key': 'b9a3e67282msh11b8dd19373171cp1a14f5jsn8651493cc570'\n",
    "# }\n",
    "# offset = 0\n",
    "# while True:\n",
    "#         params = {\n",
    "        \n",
    "#             'limit':10,\n",
    "#             'offset':offset\n",
    "#         }\n",
    "        \n",
    "#         response = requests.get(url,headers=headers,params=params)\n",
    "#         try:\n",
    "#             total_count = response.json()['metadata']['totalCount']\n",
    "#         except KeyError:\n",
    "#             continue\n",
    "            \n",
    "#         if offset < total_count:\n",
    "#             try:\n",
    "#                 tmp_df = pd.DataFrame(response.json()['data'])\n",
    "#                 df = pd.concat([df,tmp_df],ignore_index=True)\n",
    "#                 offset+=10\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#         else:\n",
    "#             break\n",
    "        \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df = pd.DataFrame()\n",
    "cities_data = []\n",
    "url = 'https://wft-geo-db.p.rapidapi.com/v1/geo/cities'\n",
    "headers = {\n",
    "    'x-rapidapi-host': 'wft-geo-db.p.rapidapi.com',\n",
    "    'x-rapidapi-key': 'ec5affb8eamshbc4783d269e875ap1a654ajsnb7e5295988f1'\n",
    "}\n",
    "offset = 0\n",
    "while True:\n",
    "        params = {\n",
    "        \n",
    "            'limit':10,\n",
    "            'offset':offset,\n",
    "            'countryIds':'PK'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url,headers=headers,params=params,timeout=30)\n",
    "        print(response.status_code)\n",
    "        json_response = response.json()\n",
    "        try:\n",
    "            total_count = json_response['metadata']['totalCount']\n",
    "        except KeyError:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        if offset <= total_count:\n",
    "            try:\n",
    "                tmp_df = pd.DataFrame(json_response['data'])\n",
    "                df = pd.concat([df,tmp_df],ignore_index=True)\n",
    "                # cities_data.append(tmp_df)\n",
    "                offset+=10\n",
    "                # if offset % 1000 == 0:\n",
    "                #     df = pd.concat([df,pd.concat(cities_data,ignore_index=True)],ignore_index=True)\n",
    "                #     df.to_csv('geo_cities_partial.csv', index=False)\n",
    "                #     cities_data = []\n",
    "                \n",
    "                    \n",
    "            except KeyError:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(2)\n",
    "        \n",
    "df.to_csv('pakistan.csv')\n",
    "# if cities_data:\n",
    "#     df = pd.concat([df,pd.concat(cities_data,ignore_index=True)],ignore_index=True)\n",
    "# df.to_csv('geo_cities_partial',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymysql\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# engine = create_engine('mysql+pymysql://root:@localhost/cities')\n",
    "# df.to_sql('120_cities.sql',con=engine,if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('pakistan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V514SPZd8f63"
   },
   "source": [
    "### Problem 3:\n",
    "\n",
    "Go to this url: https://www.flipkart.com/search?q=smartphones. This is the url to find phones in flipkart website. You have to extract the below things:\n",
    "1. image url of the phone\n",
    "2. name of the image\n",
    "3. average ratings\n",
    "4. total ratings\n",
    "5. total reviews\n",
    "6. discounted price\n",
    "7. actual price\n",
    "\n",
    "Extract all the phones which are available in this website. So you have to use the pagination concept. **Also after requesting every page through the url, please wait for a while (minimum 2-3 seconds), otherwise your IP address can be banned to access the flipkart website later.**\n",
    "\n",
    "After collecting all the data, save that in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGFT4kap8iwT"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# url = 'https://www.flipkart.com/search?q=smartphones&page=1'\n",
    "# headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "\n",
    "# soup = BeautifulSoup(requests.get(url,headers=headers).text,'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_url = []\n",
    "# img_name = []\n",
    "# avg_rating = []\n",
    "# total_rating = []\n",
    "# total_review = []\n",
    "# discounted_price = []\n",
    "# actual_price = []\n",
    "\n",
    "\n",
    "# for i in soup.find_all('div',class_='_75nlfW'):\n",
    "#     img_url.append(i.find('img',class_='DByuf4').get('src'))\n",
    "#     img_name.append(i.find('img',class_='DByuf4').get('alt'))\n",
    "#     avg_rating.append(i.find('div',class_='XQDdHH').text)\n",
    "#     total_rating.append(i.find('span',class_='Wphh3N').text.strip().split('&')[0].split()[0])\n",
    "#     total_review.append(i.find('span',class_='Wphh3N').text.strip().split('&')[1].split()[0])\n",
    "#     discounted_price.append(i.find('div',class_='Nx9bqj _4b5DiR').text)\n",
    "#     actual_price.append(i.find('div',class_='yRaY8j ZYYwLA').text)\n",
    "# d  = {'Name':img_name,'Url': img_url,'Average_Rating':avg_rating,'Total_Rating':total_rating,'Total_Reviews':total_review,'Discounted_Price':discounted_price,'Actual_Price':actual_price}\n",
    "# tmp_df = pd.DataFrame(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "flipkart = pd.DataFrame()\n",
    "for j in range(1,389):\n",
    "    url = f'https://www.flipkart.com/search?q=smartphones&page={j}'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"}\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(requests.get(url,headers=headers).text,'lxml')\n",
    "    img_url = []\n",
    "    img_name = []\n",
    "    avg_rating = []\n",
    "    total_rating = []\n",
    "    total_review = []\n",
    "    discounted_price = []\n",
    "    actual_price = []\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all('div',class_='_75nlfW'):\n",
    "\n",
    "        try:\n",
    "            img_url.append(i.find('img',class_='DByuf4').get('src'))\n",
    "        except:\n",
    "            img_url.append(np.nan)\n",
    "\n",
    "\n",
    "        try:\n",
    "            img_name.append(i.find('img',class_='DByuf4').get('alt'))\n",
    "        except:\n",
    "            img_name.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            avg_rating.append(i.find('div',class_='XQDdHH').text)\n",
    "        except:\n",
    "            avg_rating.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            total_rating.append(i.find('span',class_='Wphh3N').text.strip().split('&')[0].split()[0])\n",
    "        except:\n",
    "            total_rating.append(np.nan)\n",
    "\n",
    "\n",
    "        try:\n",
    "            total_review.append(i.find('span',class_='Wphh3N').text.strip().split('&')[1].split()[0])\n",
    "        except:\n",
    "            total_review.append(np.nan)\n",
    "\n",
    "\n",
    "        try:\n",
    "            discounted_price.append(i.find('div',class_='Nx9bqj _4b5DiR').text)\n",
    "        except:\n",
    "            discounted_price.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            actual_price.append(i.find('div',class_='yRaY8j ZYYwLA').text)\n",
    "        except:\n",
    "            actual_price.append(np.nan)   \n",
    "\n",
    "            \n",
    "        # img_name.append(i.find('img',class_='DByuf4').get('alt'))\n",
    "        # avg_rating.append(i.find('div',class_='XQDdHH').text)\n",
    "        # total_rating.append(i.find('span',class_='Wphh3N').text.strip().split('&')[0].split()[0])\n",
    "        # total_review.append(i.find('span',class_='Wphh3N').text.strip().split('&')[1].split()[0])\n",
    "        # discounted_price.append(i.find('div',class_='Nx9bqj _4b5DiR').text)\n",
    "        # actual_price.append(i.find('div',class_='yRaY8j ZYYwLA').text)\n",
    "        \n",
    "    d  = {'Name':img_name,'Url': img_url,'Average_Rating':avg_rating,'Total_Rating':total_rating,'Total_Reviews':total_review,'Discounted_Price':discounted_price,'Actual_Price':actual_price}\n",
    "    tmp_df = pd.DataFrame(d)\n",
    "    flipkart = pd.concat([flipkart,tmp_df],ignore_index=True)\n",
    "    img_url.clear()\n",
    "    img_name.clear()\n",
    "    avg_rating.clear()\n",
    "    total_rating.clear()\n",
    "    total_review.clear()\n",
    "    discounted_price.clear()\n",
    "    actual_price.clear()\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipkart.to_json('flipkart.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_url = []\n",
    "# img_name = []\n",
    "# avg_rating = []\n",
    "# total_rating = []\n",
    "# total_review = []\n",
    "# discounted_price = []\n",
    "# actual_price = []\n",
    "\n",
    "\n",
    "# for i in soup.find_all('div',class_='_75nlfW')[0]:\n",
    "#     print(i.find('img',class_='DByuf4').get('src'))\n",
    "#     print(i.find('img',class_='DByuf4').get('alt'))\n",
    "#     print(i.find('div',class_='XQDdHH').text)\n",
    "#     print(i.find('span',class_='Wphh3N').text.strip().split('&')[0].split()[0])\n",
    "#     print(i.find('span',class_='Wphh3N').text.strip().split('&')[1].split()[0])\n",
    "#     print(i.find('div',class_='Nx9bqj _4b5DiR').text)\n",
    "#     print(i.find('div',class_='yRaY8j ZYYwLA').text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in soup.find('div',class_='_5OesEi'):\n",
    "#     if i.find('span') != None:\n",
    "#         print(i.find('span').text.strip().split('&')[0].split()[0])\n",
    "#         print(i.find('span').text.strip().split('&')[1].split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
